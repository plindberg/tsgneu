---
title: Upphovsrätt och språkmodellers träningsdata
date: 2026-01-12 08:49
type: fragment
lang: sv
---
(Kommentar på Facebook-inlägg om språkmodeller och upphovsrätt.)

Jag lät Gemini läsa {% link "https://arxiv.org/abs/2601.02671" %}pappret{% endlink %} och jag förstår det som att OpenAI med GPT 4.1 lyckats väldigt väl att blockera att repetera upphovsrättsskyddade verk. Eller så kunde forskarna inte hitta något jailbreak som funkade. För det verkar ha krävt rätt mycket experimenterande med jailbreaks.

Frågan är om att träna på upphovsrättsskyddade verk bör ses som ett intrång om vi antar att det är svårt nog att få modellerna att repetera verken ordagrant.

Och var går gränsen? Är det problematiskt att det går att få sin egen text omskriven som om den vore en bok i Harry Potter-serien? Att den skulle använda sig av karaktärer från böckerna?

Jag tänker på att man har upptäckt att det går att få påtagligt mer kraftfulla modeller genom att skala upp mängden träningsdata parallellt med mängden parametrar.

Upphovsrätten är något begränsat, avsett att balansera upphovspersonens rättigheter mot det allmännas – som Lawrence Lessig brukade prata om. Kanske är det här ett fall där det allmännas rättigheter väger tyngre?

Även om det på sätt och vis är riktigt att målet är att ’kunna härma all världens texter’ så handlar det sällan om att härma enskilda texter ordagrant i sin helhet – och ’härma text’ är en kraftig förenkling av vad modellerna gör.

Är det problematiska snarare att man hanterar ännu upphovsrättsskyddade verk som en allmänning och låter företag ta betalt för att använda resultatet? Eller blir det oproblematiskt eftersom det krävs sådana enorma mängder pengar att träna en språkmodell?
